{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 3b: Polynomial Regression\n",
    "Generalization, Regularisation and all that jazz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to review overfitting, model selection and regularisation. We'll be continuing with the olympics data from previous worksheet, and looking at regression models. Note that the lessons here apply equally to classification, however it's more convenient to visualise regression models, and they are also much simpler to fit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the scientific programming packages, and ensure plots are displayed inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the olympics marathon data, combining the olympic year and the winning time and as before extract both the olympic years and the pace of the winning runner into 2-dimensional arrays with the data points in the rows of the array (the first dimension). Finaly, we can plot them to check that they've loaded in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = \"\"\"1896,4.47083333333333\n",
    "1900,4.46472925981123\n",
    "1904,5.22208333333333\n",
    "1908,4.1546786744085\n",
    "1912,3.90331674958541\n",
    "1920,3.5695126705653\n",
    "1924,3.8245447722874\n",
    "1928,3.62483706600308\n",
    "1932,3.59284275388079\n",
    "1936,3.53880791562981\n",
    "1948,3.6701030927835\n",
    "1952,3.39029110874116\n",
    "1956,3.43642611683849\n",
    "1960,3.2058300746534\n",
    "1964,3.13275664573212\n",
    "1968,3.32819844373346\n",
    "1972,3.13583757949204\n",
    "1976,3.07895880238575\n",
    "1980,3.10581822490816\n",
    "1984,3.06552909112454\n",
    "1988,3.09357348817\n",
    "1992,3.16111703598373\n",
    "1996,3.14255243512264\n",
    "2000,3.08527866650867\n",
    "2004,3.1026582928467\n",
    "2008,2.99877552632618\n",
    "2012,3.03392977050993\"\"\"\n",
    "\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    import io # Python3\n",
    "    olympics = np.genfromtxt(io.BytesIO(csv.encode()), delimiter=\",\")\n",
    "else:\n",
    "    from StringIO import StringIO  # Python2\n",
    "    olympics = np.genfromtxt(StringIO(csv), delimiter=',') #Python 2\n",
    "    \n",
    "#print(olympics) \n",
    "x = olympics[:, 0:1]\n",
    "y = olympics[:, 1:2]\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion** We will consider regression models of varying complexity, from a simple linear model to polynomial models of varying order. Based on the Olympic marathon data, what order model do you think is going to perform the best? In making your decision, think about the *interpolation* predictions for years between Olympics (e.g., 2015), and *extrapolations* into the future, e.g., 2016, 2020, 2040, etc? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We studied *linear regression* in the previous worksheet, which learns a linear function of the input. Now we will consider a more complex polynomial function. We can do this by augmenting our input representation. Where before we had instances of the form ]\n",
    "$$\\phi(\\mathbf{x}) = [ 1~ x ]$$ \n",
    "now we will be using e.g., \n",
    "$$\\phi(\\mathbf{x}) = [ 1 ~x~ x^2~ x^3~ x^4]$$ \n",
    "for a quartic model, and correspondingly enlarged weight vector. Each element $w_i$ of the weight vector corresponds to the coefficient of the input year raised to the $i^{th}$ power. We will consider a range of polynomial models of different orders. \n",
    "\n",
    "To implement this we will use *basis functions* which provide a neat way of representing our data instances such that we can still use all the linear models to acheive learn a non-linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is plot the training error for the polynomial fit. To do this let's set up some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data = x.shape[0]\n",
    "num_pred_data = 100 # how many points to use for plotting predictions\n",
    "x_pred = linspace(1890, 2016, num_pred_data)[:, None] # input locations for predictions\n",
    "order = 4 # The polynomial order to use.\n",
    "print ('Num of training samples: ',num_data)\n",
    "print('Num of testing samples: ',num_pred_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's build the *basis* matrices $\\Phi$ to represent the training data, where each column is raising the input year $X$ to various powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi = np.zeros((num_data, order+1))\n",
    "Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "for i in range(0, order+1):\n",
    "    Phi[:, i:i+1] = x**i\n",
    "    Phi_pred[:, i:i+1] = x_pred**i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve for the regression weights and make predictions both for the training data points, and the test data points. That involves solving the linear system given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi' \\Phi \\mathbf{w} = \\Phi' \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with respect to $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solve the linear system\n",
    "w =  ... # over to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and using the resulting vector to make predictions at the training points and test points,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{f} = \\Phi \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this in practice we need to use basis matrices for both the predictions and the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict at training and test points\n",
    "f = ... # over to you\n",
    "f_pred = ... # over to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used to compute the sum of squared residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SSR(\\mathbf{w}) =  \\sum_{i=1}^n \\left(\\mathbf{y}_i - \\Phi_i \\mathbf{w} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the sum of squares error\n",
    "SSR = ... # over to you\n",
    "print(SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the fit and the error, let's plot the fit and the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the error and plot the predictions\n",
    "print(\"The error is: %2.4f\"%SSR)\n",
    "plt.plot(x_pred, f_pred)\n",
    "plt.plot(x, y, 'rx')\n",
    "ax = plt.gca()\n",
    "ax.set_title('Predictions for Order 5')\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('pace (min/km)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the loop structure below to compute the error for different model orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the time model to allow python to pause.\n",
    "# import the IPython display module to clear the output.\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "error_list = []\n",
    "max_order = 6\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig1=plt.figure(figsize=(15,2*max_order))\n",
    "index=1\n",
    "\n",
    "for order in range(0, max_order+1):\n",
    "    # 1. build the basis set\n",
    "    Phi = np.zeros((num_data, order+1))\n",
    "    Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "    for i in range(0, order+1):\n",
    "        Phi[:, i:i+1] = ... # paste from above\n",
    "        Phi_pred[:, i:i+1] = ... # paste from above\n",
    "    # 2. solve the linear system\n",
    "    w = ... # paste from above\n",
    "\n",
    "    # 3. make predictions at training and test points\n",
    "    f = ... # paste from above\n",
    "    f_pred = ... # paste from above\n",
    "    \n",
    "    # 4. compute the error and append it to a list.\n",
    "    SSR = ... # paste from above\n",
    "    error_list.append(SSR)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig1.add_subplot(max_order+1,2,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim((2.5, 5.5))\n",
    "    if (order ==0):\n",
    "        plt.title('Predictions for Order ' + str(order) + ' model.')\n",
    "    \n",
    "    \n",
    "    fig1.add_subplot(max_order+1,2,index+1)\n",
    "    plt.plot(np.arange(0, order+1), np.asarray(error_list))\n",
    "    plt.xlim((0, order+1))\n",
    "    plt.ylim((0, np.max(error_list)))\n",
    "    if (order ==0):\n",
    "        plt.title('Training Error')\n",
    "    index= index+2\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Training error list: ',error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Looks like a great fit. Does that mean we can stop here, our job is done? Should take these results at face value, or are we missing something? (You might want to try an order 20 or higher model, also to see if the fits continue to improve with higher order models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What do you think might happen if we try to fit an order 100 model to this data? Is this even a reasonable thing to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold Out Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error we computed above is the training error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. In hold out validation, we keep back some of the training data for assessing generalization performance. In the case of time series prediction, it often makes sense to hold out the last few data points, in particular, when we are interested in *extrapolation*, i.e. predicting into the future given the past. To perform hold out validation, we first remove the hold out set. If we were interested in interpolation, we would hold out some random points. Here, because we are interested in extrapolation, we will hold out all points since 1980. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a training set\n",
    "x_train = x\n",
    "y_train = y\n",
    "indices_hold_out = np.nonzero(x>1980)\n",
    "x_train = np.delete(x, indices_hold_out)[:,None]\n",
    "y_train = np.delete(y, indices_hold_out)[:,None]\n",
    "\n",
    "# Create a hold out set\n",
    "x_hold_out = x[indices_hold_out][:,None]\n",
    "y_hold_out = y[indices_hold_out][:,None]\n",
    "\n",
    "\n",
    "print ('Whole dataset size', x.shape)\n",
    "print('Train split size: ', x_train.shape)\n",
    "print('Test split size: ', x_hold_out.shape)\n",
    "\n",
    "# Now use the training set and hold out set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the training and hold out data, you should be able to use the code above to evaluate the model on the hold out data. Do this in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_list = []\n",
    "max_order = 6\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig1=plt.figure(figsize=(12,max_order*2))\n",
    "index = 1\n",
    "for order in range(0, max_order+1):\n",
    "    # 1. build the basis set using x_train, x_hold_out\n",
    "    Phi = np.zeros((x_train.shape[0], order+1))\n",
    "    Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "    Phi_hold_out = np.zeros((x_hold_out.shape[0], order+1))\n",
    "    for i in range(0, order+1):\n",
    "        Phi[:, i:i+1] = ... # back to you\n",
    "        Phi_hold_out[:, i:i+1] = ... # back to you\n",
    "        Phi_pred[:, i:i+1] = ... # back to you\n",
    "        \n",
    "    # 2. solve the linear system\n",
    "    w = ... # back to you\n",
    "\n",
    "    # 3. make predictions at training and test points\n",
    "    f = ... # back to you\n",
    "    f_hold_out = ... # back to you\n",
    "    f_pred = ... # back to you\n",
    "    \n",
    "    # 4. compute the error and append it to a list.\n",
    "    valid_error = ... # back to you\n",
    "    error_list.append(valid_error)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig1.add_subplot(max_order+1,2,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim((2.5, 5.5))\n",
    "    if (order ==0):\n",
    "        plt.title('Predictions for Order ' + str(order) + ' model.')\n",
    "    \n",
    "    \n",
    "    fig1.add_subplot(max_order+1,2,index+1)\n",
    "    plt.plot(np.arange(0, order+1), np.asarray(error_list))\n",
    "    plt.xlim((0, order+1))\n",
    "    plt.ylim((0, np.max(error_list)))\n",
    "    if (order ==0):\n",
    "        plt.title('Training Error')\n",
    "    index= index+2\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Holdout error list: ', error_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What is going on here? Does this match your earlier findings, or your intuition about which model order was most appropriate? Why isn't held-out error behaving the same as training error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularising the model, using ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is optional: we are unlikely to have time for it. ####\n",
    "A nice way to limit model complexity is *regularisation* where model parameters are penalised from moving to silly values. Here we consider silly as high magnitude, which means the model is getting overly confident. Can you explain why this might be a problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll use a 6th order model, which you might consider much too powerful for this simple problem. As a first step, we'll preprocess the features to ensure they are all operating in a similar range. E.g., $2000^6 >> 2000^1$, which means the weights for the 6th order features will take on radically different values to the 1st order features. To correct for this, and allow regularisation with a single constant, we'll z-score the columns of training Phi to have zero mean and unit standard deviation. This same transformation is also applied to the testing basis matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = 6\n",
    "Phi = np.zeros((x_train.shape[0], order+1))\n",
    "Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "Phi_hold_out = np.zeros((x_hold_out.shape[0], order+1))\n",
    "for i in range(0, order+1):\n",
    "    Phi[:, i:i+1] = x_train**i\n",
    "    if i > 0:\n",
    "        mean = Phi[:, i:i+1].mean()\n",
    "        std = Phi[:, i:i+1].std()\n",
    "        print(i,mean,std)\n",
    "    else: # as the first column is constant, need to avoid divide by zero \n",
    "        mean = 0\n",
    "        std = 1\n",
    "    \n",
    "    Phi[:, i:i+1] = (Phi[:, i:i+1] - mean) / std\n",
    "    Phi_hold_out[:, i:i+1] = (x_hold_out**i - mean) / std\n",
    "    Phi_pred[:, i:i+1] = (x_pred**i - mean) / std\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform training, trying out different values of the regularisation coefficient, lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_list = []\n",
    "train_error_list = []\n",
    "lambdas = [1e-10, 1e-6, 1e-4, 1e-2, 1, 100] \n",
    "order = 6\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "fig1=plt.figure(figsize=(16,order*3))\n",
    "index =1\n",
    "for l, lamba in enumerate(lambdas):\n",
    "    # 1. build the basis set using x_train, x_hold_out\n",
    "    # done above\n",
    "        \n",
    "    # 2. solve the linear system\n",
    "    w = ... # paste from above\n",
    "\n",
    "    # 3. make predictions at training and test points\n",
    "    f = ... # paste from above\n",
    "    f_hold_out = ... # paste from above\n",
    "    f_pred = ... # paste from above\n",
    "    \n",
    "    # 4. compute the error and append it to a list.\n",
    "    valid_error = ... # paste from above\n",
    "    error_list.append(valid_error)\n",
    "    train_error = ... # over to you\n",
    "    train_error_list.append(train_error)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig1.add_subplot(len(lambdas)+1,3,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim(2.5, 5.5)\n",
    "    if (l==0):\n",
    "        plt.title('Pred. for Lambda ' + str(lamba))\n",
    "    else: \n",
    "        plt.title(str(lamba))\n",
    "        \n",
    "    fig1.add_subplot(len(lambdas)+1,3,index+1)\n",
    "    plt.plot(lambdas[:l+1], np.asarray(error_list))\n",
    "    plt.xlim((min(lambdas), max(lambdas)))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0, 12)\n",
    "    if (l==0):\n",
    "        plt.title('Held-out Error (validation/testing)')\n",
    "    \n",
    "    \n",
    "    fig1.add_subplot(len(lambdas)+1,3,index+2)\n",
    "    plt.plot(lambdas[:l+1], np.asarray(train_error_list))\n",
    "    plt.xlim(min(lambdas), max(lambdas))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0, 12)\n",
    "    if (l == 0):\n",
    "        plt.title('Training Error')\n",
    "    index= index+3\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Holdout error list: ',error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What setting gives the best heldout performance? How does this relate to the training error, and can you describe whether you see evidence of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
